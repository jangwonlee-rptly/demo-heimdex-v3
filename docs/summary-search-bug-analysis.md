# Summary Search Bug: Root Cause Analysis

**Date:** 2025-01-07
**Investigator:** Claude (Heimdex Codebase Investigator)
**Issue:** Scene with exact Korean summary text not retrievable when summary weight is maxed

## Executive Summary

**Critical Finding:** Summary-based retrieval is **completely disabled** in the current system. Even with summary weight set to maximum (1.0), the system cannot retrieve scenes by their summary content because:

1. **Summary embeddings are NOT being generated** - `embedding_summary_enabled = False` in worker config
2. **Summary field is NOT indexed in OpenSearch** - BM25 lexical search does not include summary
3. **Summary weight only affects ranking** - NOT candidate generation (retrieval phase)

**Impact:** The "summary" channel weight is effectively a **no-op** - it cannot retrieve any scenes, only rerank existing candidates from other channels.

---

## Root Cause Ranked by Likelihood

### #1 ROOT CAUSE: Summary Embeddings Not Generated (CRITICAL)
**Probability:** 99%
**Severity:** CRITICAL - Complete feature failure

#### Evidence
```python
# services/worker/src/config.py:85
embedding_summary_enabled: bool = False  # Enable summary channel
```

```python
# services/worker/src/domain/sidecar_builder.py:1045
summary=None,  # Summary field not yet implemented
```

```python
# services/worker/src/domain/sidecar_builder.py:650
if self.settings.embedding_summary_enabled and summary and summary.strip():
    # This code is NEVER executed because:
    # 1. embedding_summary_enabled = False
    # 2. summary parameter is always None
```

#### What This Means
- When videos are processed, scenes get `visual_summary` **text** stored in Supabase
- But the `embedding_summary` **vector** column remains `NULL`
- The summary channel search function `search_scenes_by_summary_embedding()` filters `WHERE embedding_summary IS NOT NULL` (migration 015:230)
- **Result:** Summary channel always returns 0 candidates, regardless of weight

#### Why Summary Weight Appears to Work in UI
The UI weight slider affects **fusion weights**, which only matter **after** candidates are retrieved:

```python
# services/api/src/routes/search.py:752-762
channel_candidates, multi_dense_timings = _run_multi_dense_search(...)
# Returns: {"summary": []}  ← ALWAYS EMPTY

# services/api/src/routes/search.py:801-810
fused_results = multi_channel_minmax_fuse(
    channel_candidates=fusion_channels,
    channel_weights=fusion_weights,  # summary weight is 1.0 here
    # But summary channel has 0 candidates, so weight has no effect
)
```

The fusion logic redistributes weights across active channels when a channel is empty:

```python
# services/api/src/domain/search/fusion.py (implied behavior)
# When summary channel is empty:
# - Removes "dense_summary" from active channels
# - Redistributes its weight (1.0) to other channels
# - Final ranking uses transcript/visual/lexical only
```

---

### #2 SECONDARY CAUSE: Summary Not Indexed in OpenSearch (HIGH)
**Probability:** 100% (verified)
**Severity:** HIGH - Affects lexical fallback

#### Evidence
```python
# services/api/src/adapters/opensearch_client.py:36-92
SCENE_INDEX_MAPPING = {
    "mappings": {
        "properties": {
            "transcript_segment": {...},  # ✓ Indexed
            "visual_summary": {...},      # ✓ Indexed
            "visual_description": {...},  # ✓ Indexed
            "combined_text": {...},       # ✓ Indexed
            # ❌ NO "summary" field for scene summaries
        }
    }
}
```

The OpenSearch mapping only has **visual_summary** (old legacy field), not the dedicated scene summary field.

#### BM25 Search Query Analysis
```python
# services/api/src/adapters/opensearch_client.py:246-274
search_body = {
    "query": {
        "bool": {
            "should": [
                {
                    "multi_match": {
                        "fields": [
                            "tags_text^4",
                            "transcript_segment^3",
                            "visual_description^2",
                            "visual_summary^2",  # Old field, not scene summaries
                            "combined_text^1",
                            # ❌ No "summary" field here
                        ]
                    }
                }
            ]
        }
    }
}
```

#### What This Means
- Even if you disable dense search and use lexical-only, you cannot search by summary
- Korean queries that match summary text won't trigger BM25 hits
- Lookup soft gating (Phase 9) will fail to generate allowlist for summary-based queries

---

### #3 TERTIARY CAUSE: Data Schema Confusion (MEDIUM)
**Probability:** 80%
**Severity:** MEDIUM - Developer confusion

#### Multiple "Summary" Fields Identified

| Field Name | Location | Purpose | Status |
|------------|----------|---------|--------|
| `visual_summary` | `video_scenes` table | Legacy field - short visual description generated by LLM | ✅ Populated |
| `embedding_summary` | `video_scenes` table | Vector embedding for summary channel (v3-multi) | ❌ NULL (not generated) |
| `summary` | **NOT IN SCHEMA** | Hypothetical scene summary field | ❌ Does not exist |

#### Evidence from Code
```python
# services/worker/src/domain/sidecar_builder.py:114
visual_summary: str,  # This is the OLD field (1-sentence description)

# vs.

# services/worker/src/domain/sidecar_builder.py:576
summary: Optional[str],  # This is the NEW field (not implemented)
```

The backfill script comment confirms this:
```python
# services/worker/src/scripts/backfill_scene_embeddings_v3.py:185
summary=None,  # Summary not implemented yet
```

#### What This Means
- There is **semantic confusion** between:
  - `visual_summary` = 1-sentence visual description (exists, populated)
  - `summary` = Hypothetical comprehensive scene summary (does not exist)
- The UI displays `visual_summary` as "scene summary"
- But the search system expects a dedicated `summary` field that was never implemented

---

## End-to-End Flow Verification

### Frontend → Backend (Weight Propagation)

✅ **Frontend** (`SearchWeightControls.tsx:26-31`)
```typescript
export interface PreferencesResponse {
  channel_weights: {
    transcript: number;
    visual: number;
    summary: number;  // ✓ User sets this to 1.0
    lexical: number;
  };
}
```

✅ **Weight Resolution** (`domain/search/weights.py:21-26`)
```python
CHANNEL_MAPPING = {
    "transcript": "dense_transcript",
    "visual": "dense_visual",
    "summary": "dense_summary",  # ✓ Maps to fusion key
    "lexical": "lexical",
}
```

✅ **Search Endpoint** (`routes/search.py:752`)
```python
channel_candidates, timings = _run_multi_dense_search(...)
# Returns: {
#   "transcript": [Candidate(scene_id=..., score=0.85), ...],
#   "visual": [Candidate(scene_id=..., score=0.72), ...],
#   "summary": [],  # ❌ ALWAYS EMPTY
#   "lexical": [Candidate(scene_id=..., score=12.3), ...]
# }
```

### Dense Summary Channel (Retrieval)

❌ **Database Function** (`migrations/015:207-240`)
```sql
CREATE OR REPLACE FUNCTION search_scenes_by_summary_embedding(...)
RETURNS TABLE (id uuid, video_id uuid, similarity float)
AS $$
BEGIN
    RETURN QUERY
    SELECT vs.id, vs.video_id,
           (1 - (vs.embedding_summary <=> query_embedding))::float AS similarity
    FROM video_scenes vs
    WHERE
        vs.embedding_summary IS NOT NULL  -- ❌ ALWAYS FALSE
        AND (1 - (vs.embedding_summary <=> query_embedding)) > match_threshold
    ...
END;
$$;
```

❌ **Python Call** (`adapters/database.py:576-621`)
```python
def search_scenes_summary_embedding(self, query_embedding, ...):
    response = self.client.rpc("search_scenes_by_summary_embedding", params).execute()
    # Returns: response.data = []  ← Always empty
    results = []
    for rank, row in enumerate(response.data, start=1):
        results.append((str(row["id"]), rank, float(row["similarity"])))
    return results  # Returns []
```

### Fusion (Ranking)

✅ **Multi-Channel Fusion** (`routes/search.py:801`)
```python
fused_results = multi_channel_minmax_fuse(
    channel_candidates={
        "dense_transcript": [...],  # 200 candidates
        "dense_visual": [...],       # 180 candidates
        "dense_summary": [],         # ❌ 0 candidates
        "lexical": [...]             # 150 candidates
    },
    channel_weights={
        "dense_transcript": 0.0,
        "dense_visual": 0.0,
        "dense_summary": 1.0,  # ✓ Weight is set correctly
        "lexical": 0.0,
    },
)
# Result: No candidates to rank (summary channel is empty)
# Fusion redistributes weight to other channels
```

### Korean Text Analysis (Lexical)

✅ **OpenSearch Analyzer** (`adapters/opensearch_client.py:22-33`)
```python
"analysis": {
    "analyzer": {
        "ko_nori": {
            "type": "custom",
            "tokenizer": "nori_tokenizer",  # ✓ Korean analyzer configured
            "filter": ["lowercase"],
        },
    }
}
```

✅ **Field Mapping** (`opensearch_client.py:54-61`)
```python
"visual_summary": {
    "type": "text",
    "analyzer": "standard",
    "fields": {
        "ko": {"type": "text", "analyzer": "ko_nori"},  # ✓ Korean subfield
        "en": {"type": "text", "analyzer": "en_english"},
    },
},
```

❌ **BM25 Query** (`opensearch_client.py:262-264`)
```python
"fields": [
    "visual_summary^2",      # This searches OLD visual_summary field
    "visual_summary.ko^2",   # Korean analyzed version
    # ❌ No dedicated "summary" field for scene summaries
]
```

**Korean Analysis is Correct** - The issue is not tokenization, it's that:
1. The dedicated scene `summary` field doesn't exist in OpenSearch
2. Only `visual_summary` (legacy 1-sentence description) is indexed

---

## Minimal Fixes (Ranked by Effort)

### Fix #1: Enable Summary Embedding Generation (RECOMMENDED)
**Effort:** LOW (1 config change + data backfill)
**Impact:** HIGH (Fully enables summary search)

#### Code Changes

**File:** `services/worker/src/config.py`
```python
# Line 85: Change from False to True
embedding_summary_enabled: bool = True  # ✓ Enable summary channel
```

**File:** `services/worker/src/domain/sidecar_builder.py`
```python
# Line 1045: Pass actual summary instead of None
) = self._create_multi_channel_embeddings(
    transcript_segment=transcript_segment,
    visual_description=visual_description or "",
    tags=tags,
    summary=visual_summary,  # ✓ Use visual_summary as summary source
    scene_index=scene.index,
    language=language,
)
```

#### Data Backfill
```bash
# Backfill existing scenes with summary embeddings
docker-compose run --rm worker python -m src.scripts.backfill_scene_embeddings_v3 \
    --force-regenerate \
    --batch-size 100

# Estimated time: ~30 minutes for 10,000 scenes
# Cost: ~$5 for 10,000 scenes (OpenAI API)
```

#### Verification
```bash
# Run debug script to confirm embeddings generated
docker-compose run --rm api python /app/scripts/debug_summary_search.py \
    --summary-text "비디오는 밤의 파리..." \
    --user-id <your-user-id>

# Expected output:
# Scenes with embedding_summary vector: 100 (100%)
```

---

### Fix #2: Index Summary in OpenSearch (OPTIONAL - for lexical fallback)
**Effort:** MEDIUM (schema update + reindex)
**Impact:** MEDIUM (Enables BM25 summary search)

#### Code Changes

**File:** `services/api/src/adapters/opensearch_client.py`
```python
# Add after line 77 (inside mappings.properties)
"summary": {  # ✓ Add dedicated summary field
    "type": "text",
    "analyzer": "standard",
    "fields": {
        "ko": {"type": "text", "analyzer": "ko_nori"},
        "en": {"type": "text", "analyzer": "en_english"},
    },
},
```

**File:** `services/worker/src/adapters/opensearch_client.py`
```python
# Update index_scene() method to include summary field
def index_scene(self, scene_data: dict) -> bool:
    doc = {
        # ... existing fields ...
        "visual_summary": scene_data.get("visual_summary", ""),
        "summary": scene_data.get("visual_summary", ""),  # ✓ Index summary
        # ...
    }
```

**File:** `services/api/src/adapters/opensearch_client.py`
```python
# Line 268: Add summary fields to BM25 query
"fields": [
    "tags_text^4",
    "transcript_segment^3",
    "summary^3",        # ✓ Add summary with high boost
    "summary.ko^3",     # ✓ Korean analyzed summary
    "summary.en^3",     # ✓ English analyzed summary
    "visual_description^2",
    # ...
]
```

#### Reindex OpenSearch
```bash
# Recreate index with new mapping
docker-compose run --rm worker python -m src.scripts.reindex_opensearch \
    --drop-existing \
    --batch-size 1000

# Estimated time: ~10 minutes for 10,000 scenes
```

---

### Fix #3: Clarify Summary Field Semantics (OPTIONAL - long-term)
**Effort:** HIGH (requires product decision + schema migration)
**Impact:** LOW (developer clarity)

#### Product Decision Required
1. **Option A:** Use `visual_summary` as the summary field
   - ✅ Already populated, no data migration
   - ❌ Semantically misleading name ("visual" vs "scene")
   - ❌ Limited to 1-sentence descriptions

2. **Option B:** Add new `scene_summary` field
   - ✅ Semantically clear
   - ✅ Can support longer, richer summaries
   - ❌ Requires data migration
   - ❌ Increases LLM API costs (new field to generate)

3. **Option C:** Rename `visual_summary` → `summary`
   - ✅ Clean semantics
   - ❌ Requires database migration
   - ❌ Breaks backward compatibility

**Recommendation:** Option A (use existing `visual_summary`) for quick fix, Option B (new field) for long-term quality.

---

## Test Plan

### Unit Tests

**File:** `services/api/tests/test_summary_search.py` (NEW)
```python
import pytest
from uuid import uuid4

def test_summary_embedding_generation(sidecar_builder):
    """Test that summary embeddings are generated when enabled."""
    # Arrange
    visual_summary = "비디오는 밤의 파리에서 에펠탑이 빛나는 장면으로 시작된다..."

    # Act
    emb_transcript, emb_visual, emb_summary, metadata = (
        sidecar_builder._create_multi_channel_embeddings(
            transcript_segment="",
            visual_description="",
            tags=[],
            summary=visual_summary,
            scene_index=1,
            language="ko",
        )
    )

    # Assert
    assert emb_summary is not None, "Summary embedding should be generated"
    assert len(emb_summary) == 1536, "Should be OpenAI embedding dimension"
    assert metadata.summary is not None, "Summary metadata should exist"
    assert metadata.summary.language == "ko"


def test_summary_channel_retrieval(db, openai_client):
    """Test that summary channel can retrieve scenes after embeddings exist."""
    # Arrange: Create test scene with summary embedding
    scene_id = uuid4()
    summary_text = "에펠탑이 빛나는 밤 풍경"
    summary_embedding = openai_client.create_embedding(summary_text)

    # Insert scene with embedding_summary
    db.client.table("video_scenes").insert({
        "id": str(scene_id),
        "visual_summary": summary_text,
        "embedding_summary": summary_embedding,
        "embedding_version": "v3-multi",
        # ... other required fields
    }).execute()

    # Act: Search by similar query
    query_text = "밤의 파리 에펠탑"
    query_embedding = openai_client.create_embedding(query_text)
    results = db.search_scenes_summary_embedding(
        query_embedding=query_embedding,
        user_id=user_id,
        match_count=10,
        threshold=0.3,
    )

    # Assert
    assert len(results) > 0, "Should find scene by summary"
    assert results[0][0] == str(scene_id), "Should find exact scene"
    assert results[0][2] > 0.7, "Similarity should be high for near-identical query"


def test_summary_weight_affects_fusion(mock_multi_dense_search):
    """Test that summary weight changes final ranking when channel has candidates."""
    # Arrange: Mock channel candidates
    mock_multi_dense_search.return_value = (
        {
            "transcript": [Candidate("scene_1", 1, 0.5)],
            "visual": [Candidate("scene_2", 1, 0.6)],
            "summary": [Candidate("scene_3", 1, 0.9)],  # High summary score
            "lexical": [Candidate("scene_1", 1, 10.0)],
        },
        {"transcript": 100, "visual": 120, "summary": 80, "lexical": 90},
    )

    # Act: Run fusion with high summary weight
    fused_results = multi_channel_minmax_fuse(
        channel_candidates=mock_multi_dense_search.return_value[0],
        channel_weights={
            "dense_transcript": 0.1,
            "dense_visual": 0.1,
            "dense_summary": 0.7,  # High summary weight
            "lexical": 0.1,
        },
    )

    # Assert
    assert fused_results[0].scene_id == "scene_3", "Summary scene should rank #1"
```

### Integration Tests (Dockerized)

**File:** `scripts/test_summary_search_integration.sh` (NEW)
```bash
#!/bin/bash
set -e

echo "=== Summary Search Integration Test ==="

# Test 1: Verify config is correct
echo "[Test 1] Checking worker config..."
docker-compose run --rm worker python -c "
from src.config import settings
assert settings.embedding_summary_enabled == True, 'Summary embedding must be enabled'
print('✓ Config OK')
"

# Test 2: Process a test video and verify summary embedding generated
echo "[Test 2] Processing test video..."
TEST_VIDEO_ID=$(docker-compose run --rm api python -c "
import sys
sys.path.insert(0, '/app/src')
from adapters.database import Database
from config import settings

db = Database(settings.supabase_url, settings.supabase_service_key)
# Create test video and get ID
# ... (upload + process video)
print(video_id)
")

# Wait for processing
sleep 60

# Test 3: Verify scene has summary embedding
echo "[Test 3] Checking scene data..."
docker-compose run --rm api python /app/scripts/debug_summary_search.py \
    --video-id "$TEST_VIDEO_ID" \
    --limit 5 | grep "Has embedding_summary: True"

# Test 4: Run actual search with summary weight
echo "[Test 4] Testing summary search..."
SEARCH_RESULT=$(docker-compose run --rm api python -c "
import sys
sys.path.insert(0, '/app/src')
from adapters.database import Database
from adapters.openai_client import openai_client
from config import settings

db = Database(settings.supabase_url, settings.supabase_service_key)

# Get first scene's summary
scenes = db.client.table('video_scenes').select('visual_summary').eq('video_id', '$TEST_VIDEO_ID').limit(1).execute()
summary = scenes.data[0]['visual_summary']

# Search for it
query_emb = openai_client.create_embedding(summary[:100])  # Use first 100 chars
results = db.search_scenes_summary_embedding(
    query_embedding=query_emb,
    user_id='$USER_ID',
    video_id='$TEST_VIDEO_ID',
    match_count=10,
    threshold=0.3,
)

assert len(results) > 0, 'Should find scenes by summary'
print(f'✓ Found {len(results)} results')
")

echo "$SEARCH_RESULT"

echo "=== All Tests Passed ==="
```

### Manual Test Cases

#### Test Case 1: Exact Summary Match
```
1. Find a scene with known summary text (use debug script)
2. Copy exact summary text from database
3. Paste into search box
4. Set summary weight to 1.0, all others to 0.0
5. Expected: Scene should appear in top 3 results
6. Actual (before fix): 0 results
7. Actual (after fix): Scene ranks #1 with >0.95 similarity
```

#### Test Case 2: Semantic Summary Match
```
1. Scene summary: "비디오는 밤의 파리에서 에펠탑이 빛나는 장면..."
2. Query: "에펠탑 야경" (shorter, semantic match)
3. Set summary weight to 1.0
4. Expected: Scene appears in top 10 (lower similarity ~0.6-0.7)
5. Actual (before fix): 0 results
6. Actual (after fix): Scene in top 5 with ~0.65 similarity
```

#### Test Case 3: Weight Redistribution
```
1. Query: "파리 에펠탑"
2. Weights: transcript=0.0, visual=0.0, summary=1.0, lexical=0.0
3. Scenario A (before fix): Summary channel empty → weights redistributed → uses other channels
4. Scenario B (after fix): Summary channel has candidates → uses only summary for ranking
5. Verify: Check response.channels_active and response.weight_source in debug mode
```

---

## Summary of Findings

| Finding | Status | Impact | Fix Effort |
|---------|--------|--------|------------|
| **Summary embeddings not generated** | ❌ CRITICAL | Complete feature failure | LOW |
| **Summary not indexed in OpenSearch** | ❌ HIGH | Lexical fallback broken | MEDIUM |
| **Weight propagation works correctly** | ✅ OK | N/A | N/A |
| **Korean analyzer configured** | ✅ OK | N/A | N/A |
| **Fusion logic redistributes empty channels** | ✅ OK (working as designed) | N/A | N/A |
| **Data schema clarity** | ⚠️  MEDIUM | Developer confusion | HIGH |

---

## Recommended Action Plan

### Phase 1: Immediate Fix (This Week)
1. ✅ Change `embedding_summary_enabled = True` in worker config
2. ✅ Update `sidecar_builder.py` to pass `visual_summary` as `summary` parameter
3. ✅ Run backfill script for existing scenes
4. ✅ Run integration test to verify fix

**ETA:** 2-3 hours + backfill time
**Risk:** LOW (additive change, no breaking changes)

### Phase 2: Lexical Enhancement (Next Sprint)
1. Add `summary` field to OpenSearch mapping
2. Update indexing logic to populate summary field
3. Add summary to BM25 query with high boost (^3)
4. Reindex all documents

**ETA:** 4-6 hours
**Risk:** MEDIUM (requires index rebuild)

### Phase 3: Product Alignment (Future)
1. Product decision on summary field semantics
2. If needed: Add dedicated `scene_summary` field
3. Update LLM prompts to generate richer summaries
4. Schema migration

**ETA:** 1-2 weeks
**Risk:** HIGH (schema change, data migration)

---

## Appendix: File References

### Key Files Analyzed

| File | Purpose | Lines Referenced |
|------|---------|------------------|
| `services/api/src/routes/search.py` | Search endpoint & fusion | 191-202, 752-762, 801-810 |
| `services/api/src/domain/search/weights.py` | Weight resolution | 18-26, 164-180, 230-327 |
| `services/api/src/adapters/opensearch_client.py` | OpenSearch schema & BM25 | 16-93, 208-304 |
| `services/api/src/adapters/database.py` | Summary embedding search | 576-621 |
| `services/worker/src/config.py` | Worker configuration | 75-85 |
| `services/worker/src/domain/sidecar_builder.py` | Embedding generation | 571-663, 1030-1060 |
| `infra/migrations/015_add_multi_embedding_channels.sql` | DB schema (summary) | 207-240 |
| `services/frontend/src/components/SearchWeightControls.tsx` | UI weight controls | 7-12, 36-42 |

### Debug Tools Created

| Tool | Purpose |
|------|---------|
| `scripts/debug_summary_search.py` | Inspect scene summary data & embeddings |
| `scripts/test_summary_search_integration.sh` | End-to-end integration test |

---

**End of Report**
